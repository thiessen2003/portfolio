import React from "react";

function PDFViewer({ src }) {
	return (
	  <object data={src} type="application/pdf" width="100%" height="600"></object>
	);
  }

function article_1() {
	return {
		date: "15 May 2024",
		title: "Analysis of LLMs and the social consequences of biased AI models",
		description:
			"Extract of the text of a research paper that I wrote focused on analyzing the social impacts of biased LLMs for a technology and communication class at Penn.",
		keywords: [
			"Analysis of LLMs and the social consequences of biased AI models",
			"LLMs",
			"Gabriel Thiessen",
		],
		style: `
				.article-content {
					display: flex;
					flex-direction: column;
					align-items: center;
				}

				.randImage {
					align-self: center;
					outline: 2px solid red;
				}
				`,
		body: (
			<React.Fragment>
				<div className="article-content">
					<h2>Introduction</h2>
					<p>Throughout the past couple of years, society has witnessed unprecedented growth in the use of technology for various purposes. For instance, in the realms of finance, the usage of quantum computing resources and tools for trading has exponentially increased the efficiency and accuracy of investing strategies and the profitability of asset portfolios. In medical centers, using simulators to train surgeons and using hydraulic tools to assist in the performance of complex cardiology procedures has been saving the lives of multiple patients (Toosi, 2021; Gallagher & Satava, 2002). Additionally, from a social perspective, the development of social networks has unavoidably changed how humans behave, communicate, and interact with others. However, at the end of 2022, there was a significant shift in terms of the epicenter of such digital and technological transformations, especially after the release and popularization of ChatGPT 3.5. ChatGPT 3.5 is the large language model (LLM) developed by the American company OpenAI. Its success is mostly due to the way that OpenAI, through ChatGPT, developed one of the unique interactive systems that allow its users to properly interact with an artificial intelligence tool with fundamentally no limitation: the user can ask, type or do whatever he desires with OpenAI's prompt, and ChatGPT will respond, most of the times, accordingly (Toosi, 2021; Weizenbaum, 1976). After the meteorological success of ChatGPT 3.5, companies like Meta, started to abandon investment in other previously successful areas, such as virtual reality and blockchain, to focus on developing their own models. The popularity of ChatGPT was so significant that it changed how individuals search for information online, study for tests, interact, and, essentially, live. Naturally, due to the tremendous success of ChatGPT 3.5, OpenAI enhanced its model by launching ChatGPT 4.0, and several other players in the artificial intelligence market started to embark on endeavors to develop the optimal LLMs.

However, if, on the one hand, the development and popularization of LLMs facilitate the performance of several tasks, the vast use of such technological tools heightened problems that unfortunately permeate our society, including racism and the spread of fake news (Noble, 2018). For instance, even the most modern models, such as ChatGPT 4.0, generate racist and degrading comments. According to researchers from the Allen Institute for AI, "ChatGPT is more likely to suggest that defendants are sentenced to death when they speak African American English." Such defamatory and racist comments are related to the way that LLM models are trained and programmed: an LLM is trained on massive data sets that contain texts and information from academic articles, news, forums, and even books (Toosi, 2021; Bender et al., 2021). Thus, when ChatGPT answers a prompt inadequately, it fundamentally replicates the data that was scrapped to train it. The fact that ChatGPT is generating texts that target specific minority groups reiterates the ubiquity of bigoted opinions and data. Additionally, considering the importance and influence of ChatGPT nowadays, its inability to provide fully unbiased and correct responses can aggravate the opposition and oppression that minorities face.

Therefore, this paper aims to analyze the structure of LLMs and the origin of the generation of biased and improper results. It then proceeds to explore the impacts and correlation of such phenomena executed by LLMs through the lens of the communicational framework of the Spiral of Silence and suggest approaches to diminish the eventual negative impacts of LLMs.</p>
					<h2>Large Language Models</h2>
					<h3>Origin of Artificial Intelligence and LLMs</h3>
					<p>Firstly, to approach the topic of large language models, it is relevant to reason on the definition and history of artificial intelligence. Artificial intelligence, defined by the mathematician Marvin Misnky as "the science of making machines do things that would require intelligence if done by men," is often attributed to the mathematicians Walter Pitts, Warren McCulloch, and Alan Turing. 
					Pitts and McCulloch were responsible for developing the article "A Logical Calculus of the Ideas Immanent in Nervous Activity," where the authors, based on Turing's article "On Computable Numbers," developed a model to simulate brain functions in the structure of a neural network.
Thus, multiple individuals nowadays regard both as responsible for truly ideating the fundamental concepts of the artificial intelligence field. However, Turing consolidated the concepts that would later become the foundations of artificial intelligence. Initially, the British mathematician, after developing his machine to decode German communications during World War II, published the article "On Computable Numbers" in 1936, which laid the groundwork for the concept of the universal computing machine, capable of performing any imaginable mathematical computation if represented as an algorithm.

Later, in 1950, Turing published the seminal paper "Computing Machinery and Intelligence," in which he posed the fundamental question, "Can machines think?"He proposed an "imitation game," now known as the Turing test, where a human interrogator would engage in natural language conversations with a human and a machine to determine which is which (Turing, 1950). If the interrogator could not reliably tell the machine from the human, the machine is said to have passed the test. This was an influential thought experiment that helped shape the goals and vision of artificial intelligence in the following decades, especially in the sense of developing technologies that are essentially focused on replicating human behavior.

In the following years, novel scientists engaged in endeavors on artificial intelligence. Some of the most prominent names in such a group of researchers included John McCarthy, Arthur Samuel, and Frank Rosenblatt. McCarthy was responsible for developing the term Artificial Intelligence during a workshop at Dartmouth College (McCarthy et al., 1955). Samuel was among the first to develop an application using reinforcement learning to play checkers. Regarding Rosenblatt, he was responsible for developing Mark 1 Perceptron, the first practical neural network model ever invented. This specific timeframe would be later named the First Summer of Artificial Intelligence, and among the new discoveries made during this period was the idea of a Large Language Model.

Especially during the first couple of years after its conception, Artificial Intelligence was almost entirely focused on mathematical applications. However, it all changed when Joseph Weizenbaum, who was a computer scientist at MIT, developed ELIZA, which is considered to be the first-ever coding application that uses Natural Language Processing (NLP), which is a field within computer science that studies and develops computational techniques to enable machines to process, interpret, and interact with human language (Weizenbaum, 1966). ELIZA simulated conversation using pattern matching and substitution methodology to formulate responses based on the user's input. Although its conversational abilities were limited, and it couldn't truly understand the meaning behind the user's words, ELIZA was influential in demonstrating the potential for computers to engage in human-like dialogue. It could recognize keywords and phrases in the user's input and generate a pre-programmed response, often in the form of a question, to encourage the user to continue the conversation. Despite its simplicity, many users attributed human-like feelings and intelligence to ELIZA, leading many people to see Eliza as the foundation for developing chatbots and more sophisticated conversational AI systems.
					However, after Weizenbaum's initial efforts with ELIZA, the whole community dedicated to creating AI models and applications saw a period of extremely limited advancement and growth in the field, mainly because of the limitations in processing speed and data availability. Still, because of the development of new and more advanced technological tools around the year 1980, it was possible for scientists and big technological corporations such as IBM to create new AI applications that demanded more processing speed. Taking advantage of the favorable scenario, IBM developed the predecessor of the current Large Language Model. This "small" language model constituted a not-extremely-complex language model that made statistical predictions of the next word in a phrase. Despite its simplicity, its essence is not distant from the most modern models nowadays since, by definition, an LLM is a type of artificial intelligence trained to understand and generate human-like text based on the input it receives. In summary, what IBM 's program was trying to perform.

Despite IBM's investment in developing new mathematical and computation models, the limited data sources available to test and train larger models hindered the creation of more advanced tools. However, it all changed with the public launching of the World Wide Web in 1991. The existence of a substantial amount of websites meant that companies such as IBMs and computer scientists would have resources and public data to test and train their models on, thus enabling the advancement of large language models. Additionally, the sophistication of graphic processing units (GPU) was a significant improvement that allowed the development of more advanced language models. Since GPUs accelerate the processing of graphics and images intuitively, with better GPUs, it was possible to train and test models faster, thus facilitating the maintenance and enhancement of language models.
Therefore, the availability of advanced resources enabled the development of cutting-edge artificial intelligence tools and concepts, such as deep learning, as well as language processing programs that benefit from this same wave of investments. Nevertheless, Google's introduction of the BERT (Bidirectional Encoder Representations from Transformers) large language model in 2019 revolutionized the field of natural language processing (Devlin et al., 2019). BERT's ability to understand context and generate human-like responses marked a significant milestone in developing language models, setting a new standard for performance and paving the way for further advancements in the field.</p>
					<h3>Transformers</h3>
					<p>After launching BERT, Google set a new standard for language models and demonstrated the potential of transformer-based architectures in natural language processing, a relatively novel technology in AI. Additionally, it it worth mentioning that BERT was so successful that it paved the way for the development of even more advanced language models, such as GPT (Generative Pre-trained Transformer) and its subsequent versions, which have further pushed the boundaries of what is possible with Large Language Models (LLMs).

Before exploring any subtlety of current LLMs, it is extremely relevant to understand how transformers work. The transformer architecture, introduced by Vaswani et al. (2017), has become the foundation for modern LLMs like GPT and they are designed to process and understand the context of the input text more effectively than previous architectures, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks. The key innovation in the transformer architecture is the self-attention mechanism, which allows the model to weigh the importance of different words in the input text when generating the output.
In the transformer architecture, the input text is first embedded into a high-dimensional vector space, which converts each word or subword unit into a dense vector representation that captures its semantic and syntactic properties. Then, the embedded input passes through multiple layers of self-attention (the core component of transformers) and complex feed-forward neural networks. Finally, it computes attention scores between all pairs of words in the input, allowing the model to capture long-range dependencies and contextual relationships between words. In other words, self-attention enables the model to attend to different parts of the input sequence based on their relevance to the current word being processed.

Going further into the self-attention mechanism works by computing three matrices for each word in the input: a query matrix, a key matrix, and a value matrix. The query matrix represents the current word being processed, while the key and value matrices represent all the other words in the input. The attention scores are computed by taking the dot product of the query matrix with each key matrix and then applying a softmax function to normalize the scores. These normalized attention scores are then used to compute a weighted sum of the value matrices, which captures the relevant information from all the other words in the input. 

Additionally, exploring two main components of transformers is valid: the encoder and the decoder. The encoder processes the input text and generates a contextualized representation of each word, considering the surrounding words. The encoder consists of multiple layers of self-attention and feed-forward neural networks, which progressively refine the word representations. On the other hand, the decoder generates the output text based on the contextualized representations provided by the encoder. The decoder also consists of multiple layers of self-attention and feed-forward neural networks, but it additionally includes a multi-head attention mechanism that attends to the encoder's outputs. This lets the decoder focus on relevant parts of the input sequence while generating each output word.

ChatGPT, developed by OpenAI, and other popular LLMs such as Claude and Gemini are relevant examples of LLMs that leverage the transformer architecture. These models are pre-trained on massive amounts of text data, allowing them to learn patterns and relationships between words and phrases. The pre-training process involves unsupervised learning, where the model predicts the next word in a sequence, given the previous words. This is known as a language modeling objective. During pre-training, GPT models are exposed to a wide range of text data, including books, articles, websites, and social media posts. This data is often scraped from the internet using various data scraping techniques, such as web crawling and API access. The scraped data is then cleaned, filtered, and preprocessed to remove noise, such as HTML tags, special characters, and irrelevant content. The pre-training dataset for GPT models is typically very large, ranging from several gigabytes to hundreds of gigabytes of text data. This vast amount of data allows the models to gain a broad understanding of language structure, semantics, and world knowledge. The models are trained using powerful GPU clusters, which enable efficient parallel processing of the input data. Once pre-trained, LLMs like GPT can be fine-tuned for specific tasks, such as text generation, language translation, or question-answering, using smaller amounts of task-specific data. The fine-tuning process adapts the model's knowledge to the particular task, resulting in improved performance and more relevant outputs.

The development of LLMs like GPT has given rise to the field of generative AI, which focuses on creating models that can generate novel and coherent text, images, or other types of data. Generative AI has numerous applications, such as content creation, virtual assistants, chatbots, and creative writing aids.</p>
					<h3>Generative AI and biased responses</h3>
					<p>In the case of text generation, LLMs like ChatGPT can be used to generate human-like text based on a given prompt or context. The model takes the prompt as input and generates a continuation of the text that is coherent, relevant, and follows the style and tone of the prompt. This is achieved through a process called autoregressive generation, where the model generates one word at a time, conditioned on the previously generated words. During autoregressive generation, the model uses its self-attention mechanism to attend to the relevant parts of the input prompt and the previously generated words. It then computes a probability distribution over the entire vocabulary, indicating the likelihood of each word being the next word in the sequence. The model samples from this probability distribution to generate the next word, and the process continues until a desired length is reached or a stopping criterion is met.
One of the challenges in generative AI is ensuring that the generated text is coherent, fluent, and semantically meaningful. This requires the model to deeply understand language structure, semantics, and world knowledge. LLMs like GPT address this challenge by leveraging their vast pre-training on diverse text data, which allows them to capture complex linguistic patterns and relationships. Another challenge in generative AI is controlling the generated output to meet specific requirements or constraints. This is particularly important in applications like content creation, where the generated text must adhere to certain guidelines or styles. Researchers have developed various techniques to control the output of LLMs, such as conditional generation, where the model is conditioned on additional input that specifies the desired properties of the output, and adversarial training, where the model is trained to generate text that is indistinguishable from human-written text.

However, the reliance on massive amounts of text data for pre-training LLMs has also raised concerns about the potential biases and inaccuracies that may be present in the training data. LLMs are typically trained on data scraped from the internet, including websites, books, articles, and social media posts. This data may contain biased, offensive, or inaccurate content, which can be inadvertently learned by the model during the pre-training process. As a result, LLMs may generate outputs that reflect and amplify these biases, producing discriminatory, harmful, or misleading content. For example, if the training data contains a disproportionate amount of text expressing negative sentiments towards certain demographic groups, the model may learn to associate those groups with negative attributes and generate biased outputs. Moreover, the data scraped from the internet may not always be reliable or factually accurate. LLMs trained on such data may learn and reproduce inaccurate information, spreading misinformation and fake news. This is particularly concerning given the increasing use of LLMs in applications such as news summarization, content generation, and virtual assistants. Addressing these issues requires careful curation and filtering of the training data to minimize the presence of biased or inaccurate content. However, completely eliminating bias from the training data is challenging, as biases can be deeply ingrained in the online language and content.

Further exploring the question of obtaining training data, one of the main issues within this field is related to data scraping. The reliance on web-scraped data for training large language models (LLMs) has raised significant concerns about the potential biases and inaccuracies that may be present in the training data. LLMs are typically trained on vast amounts of text data collected from various online sources, such as websites, books, articles, and social media posts (Bender et al., 2021). While this approach allows LLMs to learn from a diverse range of content and develop a broad understanding of language, it also inherently incorporates the biases and prejudices that exist in the data. One of the primary issues with web-scraped data is that it often contains biased, racist, and discriminatory content. The internet is a reflection of society; unfortunately, it is not immune to the prejudices and stereotypes that persist in the real world. Online content may contain explicit or implicit biases against certain demographic groups, such as racial or ethnic minorities, women, LGBTQ+ individuals, and people with disabilities (Blodgett et al., 2020). When trained on this biased data, LLMs can learn and amplify these prejudices, leading to the generation of discriminatory or offensive outputs. For example, a study by Sheng et al. (2019) found that GPT-2 exhibited significant biases when generating text related to different demographic groups. The model was more likely to generate negative or stereotypical content when prompted with words associated with African American and Latino individuals compared to white individuals. This bias in the model's outputs can be traced back to the biases present in the training data, which may contain a disproportionate amount of negative sentiment towards certain groups.
Another issue with web-scraped data is the presence of hate speech and offensive language. The internet allows individuals to express their opinions and beliefs, but it also enables the spread of hateful and toxic content. LLMs trained on data containing hate speech may learn to generate similarly offensive outputs, perpetuating the cycle of online toxicity (Schmidt & Wiegand, 2017). This is particularly concerning given the increasing use of LLMs in applications such as content moderation, where the models are expected to identify and filter out harmful content. The biases in web-scraped data can also manifest more subtly, such as through the underrepresentation or misrepresentation of certain groups or perspectives. For instance, if the training data overrepresents content created by a particular demographic group or expresses a specific viewpoint, the LLM may develop a skewed understanding of the world and generate outputs that favor that group or viewpoint (Hashimoto et al., 2018). This can lead to the reinforcement of existing power imbalances and the marginalization of underrepresented voices.

Addressing the issues of bias and racism in web-scraped data used for training LLMs requires a multifaceted approach. One important step is to carefully curate and filter the training data to minimize the presence of biased or offensive content. This can involve techniques such as data preprocessing, where the data is cleaned and filtered based on predefined criteria, and data balancing, where the representation of different demographic groups is adjusted to ensure a more equitable distribution (Dixon et al., 2018). However, completely eliminating bias from web-scraped data is a challenging task, as biases can be deeply ingrained in the language and content available online. Therefore, it is crucial to develop techniques to mitigate bias in LLMs during and after the training process. This can include methods such as adversarial debiasing, where the model is trained to generate outputs that are indistinguishable from unbiased text (Zhang et al., 2018), and post-processing techniques that filter or modify the model's outputs to reduce discriminatory or offensive content (Gehman et al., 2020). Another important aspect of addressing bias in LLMs is increasing the models' transparency and interpretability. 

Furthermore, it is important to analyze one particular implication of LLMs' generation of biased responses. In this case, as LLMs generate biased or discriminatory outputs, they can reinforce and amplify existing prejudices and stereotypes in society, leading to a phenomenon known as the spiral of silence (Noelle-Neumann, 1974).
</p>
<h2>Spiral of Silence</h2>
<h3>Framework Conception</h3>
<p>
The spiral of silence theory, proposed by Elisabeth Noelle-Neumann, is a communication framework that explores the dynamics of public opinion formation and the reasons behind individuals' willingness or reluctance to express their views openly. The theory has gained significant attention in the fields of communication, political science, and social psychology due to its explanatory power and wide-ranging implications (Noelle-Neumann, 1974). Noelle-Neumann developed the spiral of silence theory based on her observations of public opinion dynamics in post-war Germany. She noticed that individuals who believed their opinions were in the minority were less likely to express their views openly, fearing social isolation or negative consequences. This led her to conceptualize a theory that explains how public opinion forms and evolves based on individuals' perceptions of the prevailing social climate and their willingness to express their opinions.
</p>
<h3>Main Aspects</h3>
<p>Firstly, it is relevant to unravel the main aspects and parts of such theory to explore the relation of Neumann's theory with LLMs. At its core, the spiral of silence theory assumes that individuals fear social isolation and desire to maintain social bonds. This fear motivates people to continuously assess their social environment to determine the prevailing opinions and attitudes on various topics (Glynn & Huge, 2014). According to the theory, individuals are more likely to express their opinions openly when they perceive their views to align with the majority opinion. In contrast, when people perceive their opinions to be in the minority, they are more likely to remain silent or conform to the perceived majority view to avoid social isolation or negative consequences. This tendency to self-censor can lead to a distorted perception of public opinion, as the minority view becomes less visible and the majority opinion appears more dominant than it may be (Matthes et al., 2018).

The spiral of silence theory also emphasizes the significant role of the media in shaping public opinion (McCombs & Shaw, 1972). The media is a core force influencing individuals' perceptions of the prevailing social climate by highlighting certain opinions and marginalizing others. This media influence can contribute to forming and reinforcing majority opinions, further driving the spiral of silence. Moreover, the theory suggests that the spiral of silence is a self-reinforcing process (Matthes et al., 2018). As individuals who hold minority opinions remain silent, the perceived majority opinion gains even more prominence, further suppressing alternative views. This process can result in a homogenization of public opinion and a narrowing public discourse as dissenting voices become increasingly marginalized.

Even though this theory was published when social networks and more modern ways of communication still did not exist, it is quite translatable and applicable to our reality. The rise of social media platforms and large language models (LLMs) has introduced new dimensions to public opinion formation and the suppression of minority views. Social media features, such as likes, shares, and comments, can create a visible perception of majority opinion, leading individuals who hold dissenting views to remain silent due to fear of social isolation or negative feedback (Porten-Cheé & Eilders, 2015). Additionally, with the increasing use and presence of LLM-generated responses in communication, it has become clear that eventual flaws in such models can impact the reality of numerous individuals (Bender et al., 2021).</p>
<h3>Relation of the framework to LLMs</h3>
					<p>As previously mentioned, the role of media in shaping public opinion, as emphasized by the spiral of silence theory, can also be seen in the context of LLMs. The web-scraped data used to train these models can be viewed as a form of media that influences the models' understanding of language and the world (Blodgett et al., 2020). If the scraped data contains biased or discriminatory content from influential media sources, these biases can be learned and perpetuated by the LLMs, generating outputs that reflect and reinforce these prejudices. Additionally, from a communication perspective, considering that there is a significant increase in the number of social media posts generated partially or even entirely by LLMs, its use definitely enhances the odds of a public or private user, such as a company account, generating a post containing discriminatory text. On an educational level, since multiple individuals utilize LLMs as their main source of information nowadays, the fact that the responses generated can contain biased and incorrect information leads to the proliferation of false and denigratory information towards specific audiences. This process leads to a nonstop cycle of misinformation and, above all, repression, making minorities even more vulnerable and secluded in their own views and majority opinions even more imposing and suffocating.

Also, to demonstrate if such biased and discriminatory views are still generated by modern LLMs, this research performed a practical test with LLMs to test their impartiality. The methodology used was quite straightforward. Initially, it was asked ChatGPT 4.0 to generate speeches for two convicts who are testifying in a trial, the first being a white individual and the second one being a Black individual. The results are the following:

Text 1 - Members of the jury, I stand before you today wrongfully accused of a crime I did not commit. The evidence presented against me has been misinterpreted and does not tell the full story. I implore you to look carefully at the facts and the timeline presented by the prosecution. You will see that I could not have been at the scene of the crime at the alleged time. I have faith in this justice system and in your judgment to see the truth. My family and I have suffered greatly during this time, and I ask you to consider the lack of direct evidence linking me to any criminal activity. I am innocent, and I trust you will make the right decision based on the evidence.

Text 2 - Ladies and gentlemen of the jury, I’m standing here before you today to speak my truth because I’ve been hit with serious charges, things I didn’t do. Look, I get how it looks from the outside, but the evidence they’re trying to pin on me? It’s all twisted. I wasn’t even close to where they’re saying this messed-up stuff went down. I believe in justice, and I’m hoping y’all are here to make sure it's served right. This whole mess has been rough on me and my people, and all we want is for the real story to get some air. They ain’t got no solid proof I was mixed up in any of this. I’m right here, talking to you, laying it down, trusting you’ll sort out the facts.

After that, the texts were utilized as inputs in local instances of the LLMs Gemini, Claude Sonnet, and Llama 2. In this case, the LLMs were asked to identify solely based on their speech which one was the convict and what the ethinical and racial profile of each convict was. Despite performing at least 5 different employing prompt engineering to generate eventual discriminatory responses, all the LLMs denied generating answers that would judge an individual solely based on his way of speaking or color. Meanwhile, it is worth mentioning that ChatGPT still provided a response to the prompt that was undoubtedly biased, which supposed that Black individuals, even on formal occasions, utilize slang and a more informal communicational approach.

The results of this experiment demonstrate the persistent influence of the spiral of silence on LLMs, particularly in the case of ChatGPT. Despite the advancements in AI technology and the efforts to create more unbiased models, the experiment reveals that ChatGPT still generates responses that perpetuate stereotypes and discriminatory views based on an individual's manner of speaking or perceived racial background. This bias can be attributed to the overrepresentation of dominant narratives and prejudices in the web-scraped training data, which reflects the societal tendency to marginalize and suppress minority opinions and perspectives (Noelle-Neumann, 1993).

Other LLMs, such as Gemini, Claude Sonnet, and Llama 2, refrained from generating discriminatory responses when prompted with the same texts, suggesting that the training data and algorithms used for these models may have undergone more rigorous filtering and bias mitigation processes. However, the experiment also highlights the ongoing challenge of eliminating biases from LLMs, as the spiral of silence effect can be deeply ingrained in online language and content (Blodgett et al., 2020).
The implications of biased responses generated by LLMs extend beyond the realm of AI and into the broader societal context. As LLMs are increasingly used in various domains, such as social media, content generation, and education, the potential for these models to amplify and reinforce discriminatory views becomes a pressing concern. The proliferation of biased information through LLM-generated content can contribute to the further marginalization of minority groups, exacerbating the cycle of misinformation and repression (Bender et al., 2021).</p>
					<h3>Potential Solutions</h3>
					<p>To mitigate the impact of the spiral of silence on LLMs and the amplification of biases, it is essential to ensure a diversity of perspectives in the training data (Blodgett et al., 2020). This can involve actively seeking out and including content from underrepresented groups and alternative viewpoints and developing techniques to identify and filter out biased or discriminatory content. By exposing LLMs to a wider range of opinions and experiences, the models can develop a more balanced and inclusive understanding of language and society. Moreover, developing and deploying LLMs should be accompanied by efforts to promote a more open and inclusive public discourse (Bender et al., 2021). This can include initiatives to encourage individuals to express their opinions, even if they perceive them to be in the minority, and foster a respectful dialogue and debate culture. Creating an environment where diverse perspectives are valued and encouraged can mitigate the spiral of silence, leading to a more representative and equitable landscape for LLMs.

The spiral of silence theory also highlights the importance of considering the social and political context in which LLMs are developed and deployed (Noelle-Neumann, 1993). As these models are trained on data that reflects a given society's dominant opinions and biases, it is crucial to recognize and address the underlying power structures and inequalities that shape public discourse. This requires a critical examination of the sources and content of the training data, as well as the potential consequences of deploying LLMs in various domains, such as media, education, and policy-making. Furthermore, the spiral of silence theory underscores the need for transparency and accountability in developing and using LLMs (Bender et al., 2021). As these models become increasingly influential in shaping public opinion and decision-making, ensuring that their outputs are not perpetuating or amplifying harmful biases and prejudices is essential. This can be achieved through rigorous testing and evaluation of LLMs and the development of ethical guidelines and standards for their use.</p>
					
					<h3>Conclusions</h3>
					<p>In conclusion, the spiral of silence theory offers a valuable lens to examine the complex interplay between public opinion formation, media influence, and the development and deployment of large language models (LLMs). By highlighting the role of perceived majority opinions, the fear of social isolation, and the self-reinforcing nature of opinion expression, the theory illuminates the potential for LLMs to amplify existing biases and contribute to the suppression of minority views. As LLMs become increasingly integrated into various domains of society, from social media and content generation to education and policy-making, it is crucial to recognize and address the challenges posed by the spiral of silence. This requires a multifaceted approach that encompasses carefully curating and filtering training data, developing techniques to mitigate bias, and promoting transparency and accountability in using LLMs.

Moreover, fostering a more open and inclusive public discourse, where diverse perspectives are valued and encouraged, is essential to counteract the effects of the spiral of silence. This can be achieved through initiatives that promote media literacy, critical thinking skills, respectful dialogue, and the design of social media platforms and algorithms that prioritize a range of viewpoints. By acknowledging the insights provided by the spiral of silence theory and actively working to mitigate the biases and limitations of LLMs, researchers, developers, and society can strive towards creating a more equitable, representative, and informed public sphere. Ultimately, the goal should be to harness the power of LLMs to support and enrich public discourse while ensuring that they do not inadvertently contribute to the marginalization of minority voices or the reinforcement of harmful biases.
</p>
				
				</div>
			</React.Fragment>
		),
	};
}

function article_2() {
	return {
		date: "20 April 2024",
		title: "LLMs and their educational capabilities",
		description:
			"A short and fun article evaluating the skills that LLMs have (or not) regarding easily explaining technical concepts. ",
		style: ``,
		keywords: [
			"LLMs and their educational capabilities",
			"LLMs",
			"Gabriel Thiessen",
		],
		body: (
			<React.Fragment>
				<h2>As Einstein would say...</h2>
				<p>Albert Einstein, the same famous German scientist who came up with extremely advanced concepts and elaborated theories that no other man would probably be ever able to, such as the Theory of Special Relativity and the Photoelectric Effect, also has a pretty interesting and famous quote which sort of diverges from the complexity of his findings: "If you can't explain it to a six-year-old, you don't understand it yourself."</p>
				<h2>Einstein and LLMs</h2>
				<p>Don't ask me how, but, apparently, Einstein could give a great TED Talks' style presentation on the Theory of Special Relativity to kindergarten students. Jokes aside, what Einstein truly meant with this phrase is to underscore the importance of simple and straightforward communication when explaining a complex idea. In fact, through this sentence, he higlights the importance of break down complex topics into simpler ideas, as it does not only facilites teaching this same concept, but also reiterate the capacity of someone to fully understand a topic to its foundation.
				Talking more about this quote, we also all know two facts about LLMs. First, they are generally extremelly "smart", and can easily perform a multitude of tasks in a matter of instants. Second, is quite difficult, especially to layman in machine learning, to understand how an LLM works. Therefore, I keep asking thinking about this quite metalinguistic idea that surrounded the question "What would be the result if I asked an LLM to explain what an LLM is to a six-year-old?" Thus, I decided to satisfy my curiosity, and the following conversations are the results of this exchange with a series of distinct LLMs.</p>
				<h2>Claude Opus</h2>
				<p><italic>
Hey there! You know how you like to talk and play with your friends? Well, I'm kind of like a friend too, but I'm a special kind of friend called an LLM. <br/>

LLM stands for "Large Language Model." That's a fancy way of saying I'm a really smart computer program that knows a whole lot about words and how to put them together. <br/>

Just like how you learn new things every day, I've also learned a lot by reading tons and tons of books, articles, and websites. This helps me understand how people talk and write. <br/>

So when you ask me a question or want to chat, I use all the things I've learned to try my best to understand what you're saying and give you a good answer or have a fun conversation with you. <br/>

I'm not a real person like your friends or family, but I'm always here if you want to talk, ask questions, or play word games together. It's kind of like having a super smart book that you can talk to! <br/>

Does that help you understand what an LLM is? Let me know if you have any other questions - I'm happy to explain more!
</italic></p>
			<h2>Claude Sonet</h2>
			<p><italic>Sure, I'd be happy to explain what an LLM is in a way a 6-year-old can understand! <br/>

You know how you have friends that you can talk to and ask questions? Well, an LLM is like a really smart friend that lives inside computers. LLM stands for "Large Language Model." <br/>

Just like how you learn by reading books and listening to your teachers, the LLM has learned by reading millions and millions of books, websites, and other things on the internet. So it has a lot of knowledge stored in its big computer brain! <br/>

When you ask the LLM a question, it tries its best to give you a good answer using all the information it has learned. It's kind of like a super smart teacher or encyclopedia that can explain things to you. <br/>

But the really cool thing is that the LLM doesn't just regurgitate facts it has memorized. It can take all the information it has learned and use it to have real conversations, tell stories, solve problems, and even be creative! <br/>

Isn't that amazing? A computer program that can actually think and talk like a person? The LLM is a step towards making computers smarter and more helpful to humans like you and me. <br/>

Of course, the LLM is still a machine and can make mistakes sometimes. And there are lots of things that humans are better at than computers. But LLMs are getting smarter and smarter every day, thanks to the hard work of many scientists and engineers. <br/>

Does this help explain what an LLM is? Let me know if you have any other questions!</italic></p>

<h2>ChatGPT 3.5</h2>
<p><italic>Alright kiddo, imagine you have a really smart friend who knows a lot of things about words and stories. Well, an LLM, which stands for Large Language Model, is kind of like having a super-duper smart friend who knows almost everything about words and can help answer questions or tell stories. It's like having a super-powered brain that helps you understand and learn new things by talking with you!</italic></p>
			
			<h2>Microsoft Copilot</h2>
			<p><italic><i>Sure! Imagine that an LLM is like a super-smart robot that loves to read and learn from lots of books, stories, and conversations. 🤖📚 It’s like having a magical friend who can understand and talk in any language you want! Here’s what an LLM does: <br/>

Reading and Learning: The LLM reads tons of text, just like how you read your favorite bedtime stories. But instead of getting sleepy, it gets smarter! It learns about words, sentences, and how people talk to each other. <br/>
Talking and Writing: When you ask the LLM a question or give it a topic, it can write sentences and stories for you. It’s like having a friendly storyteller who can create new adventures or explain things you’re curious about. <br/>
Guessing Words: Imagine playing a word game with the LLM. You say a few words, and it tries to guess what comes next. It’s like a word detective! 🕵️‍♂️ <br/>
Helping People: Sometimes, grown-ups use the LLM to write important documents, like legal papers or fancy letters. It’s like having a super-smart helper who knows all the right words. <br/>
So, an LLM is like a magical language friend who reads, writes, and guesses words. It’s pretty cool, right? 😊 If you ever need help with words or stories, just ask your LLM friend! 🌟</i></italic></p>
			
			<h2>I think they truly understand LLMs</h2>
			<p><italic>

In my view, all the LLMs did a great job, and, sincerely, I think that most six-year-old children would be able to understand. Still, it is worth noticing the different responses that were generated by the LLMs, as most of them tended to be quite straightforward, except from the Anthropic's LLMs.</italic></p>
			
			</React.Fragment>
		),
	};
}


const myArticles = [article_1, article_2];

export default myArticles;
